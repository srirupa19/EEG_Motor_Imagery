{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4415,"status":"ok","timestamp":1688948260187,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"6o30KhGTT_Ep","outputId":"f90d38fc-6a67-4411-8137-cfcab5bb4035"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"jJbmqaHGUSn1"},"source":["#Preprocessing\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3280,"status":"ok","timestamp":1688948263459,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"5KWjnbGuUXIp","outputId":"551d9d61-38dc-41fa-bf32-83c9ccc27fd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.4)\n"]}],"source":["!pip install h5py"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1688948263460,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"s4nYPT0yUTIj"},"outputs":[],"source":["import h5py\n","import numpy as np\n","\n","\n","def import_data(every=False):\n","    if every:\n","        electrodes = 25\n","    else:\n","        electrodes = 22\n","    X, y = [], []\n","    for i in range(9):\n","        A01T = h5py.File('/content/drive/My Drive/data/A0' + str(i + 1) + 'T_slice.mat', 'r')\n","        X1 = np.copy(A01T['image'])\n","        X.append(X1[:, :electrodes, :])\n","        y1 = np.copy(A01T['type'])\n","        y1 = y1[0, 0:X1.shape[0]:1]\n","        y.append(np.asarray(y1, dtype=np.int32))\n","\n","    for subject in range(9):\n","        delete_list = []\n","        for trial in range(288):\n","            if np.isnan(X[subject][trial, :, :]).sum() > 0:\n","                delete_list.append(trial)\n","        X[subject] = np.delete(X[subject], delete_list, 0)\n","        y[subject] = np.delete(y[subject], delete_list)\n","    y = [y[i] - np.min(y[i]) for i in range(len(y))]\n","    return X, y\n","\n","\n","def train_test_subject(X, y, train_all=True, standardize=True):\n","\n","    l = np.random.permutation(len(X[0]))\n","    X_test = X[0][l[:50], :, :]\n","    y_test = y[0][l[:50]]\n","\n","    if train_all:\n","        X_train = np.concatenate((X[0][l[50:], :, :], X[1], X[2], X[3], X[4], X[5], X[6], X[7], X[8]))\n","        y_train = np.concatenate((y[0][l[50:]], y[1], y[2], y[3], y[4], y[5], y[6], y[7], y[8]))\n","\n","    else:\n","        X_train = X[0][l[50:], :, :]\n","        y_train = y[0][l[50:]]\n","\n","    X_train_mean = X_train.mean(0)\n","    X_train_var = np.sqrt(X_train.var(0))\n","\n","    if standardize:\n","        X_train -= X_train_mean\n","        X_train /= X_train_var\n","        X_test -= X_train_mean\n","        X_test /= X_train_var\n","\n","    X_train = np.transpose(X_train, (0, 2, 1))\n","    X_test = np.transpose(X_test, (0, 2, 1))\n","\n","    return X_train, X_test, y_train, y_test\n","\n","\n","def train_test_total(X, y, standardize=True):\n","\n","    X_total = np.concatenate((X[0], X[1], X[2], X[3], X[4], X[5], X[6], X[7], X[8]))\n","    y_total = np.concatenate((y[0], y[1], y[2], y[3], y[4], y[5], y[6], y[7], y[8]))\n","\n","    l = np.random.permutation(len(X_total))\n","    X_test = X_total[l[:50], :, :]\n","    y_test = y_total[l[:50]]\n","    X_train = X_total[l[50:], :, :]\n","    y_train = y_total[l[50:]]\n","\n","    X_train_mean = X_train.mean(0)\n","    X_train_var = np.sqrt(X_train.var(0))\n","\n","    if standardize:\n","        X_train -= X_train_mean\n","        X_train /= X_train_var\n","        X_test -= X_train_mean\n","        X_test /= X_train_var\n","\n","    X_train = np.transpose(X_train, (0, 2, 1))\n","    X_test = np.transpose(X_test, (0, 2, 1))\n","\n","    return X_train, X_test, y_train, y_test\n"]},{"cell_type":"markdown","metadata":{"id":"AOanCcEWUhdA"},"source":["#Initializers_Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3328,"status":"ok","timestamp":1688941755579,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"4UNLSCDoU3Y3","outputId":"91aefff3-bba3-4f84-a82e-9680db65fa4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: preprocessing in /usr/local/lib/python3.10/dist-packages (0.1.13)\n","Requirement already satisfied: nltk==3.2.4 in /usr/local/lib/python3.10/dist-packages (from preprocessing) (3.2.4)\n","Requirement already satisfied: sphinx-rtd-theme==0.2.4 in /usr/local/lib/python3.10/dist-packages (from preprocessing) (0.2.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.2.4->preprocessing) (1.16.0)\n"]}],"source":["pip install preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aajA-KHXUhBa"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import importlib\n","import matplotlib.pyplot as plt\n","import preprocessing\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lK9OnADXU-6s"},"outputs":[],"source":["X, y = import_data()\n","X_train,X_test,y_train,y_test = train_test_total(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA8VI3_MU-9Y"},"outputs":[],"source":["def rnn_model(features, labels, mode, params):\n","\n","\n","  num_hidden_layers = len(params['hidden_layers'])\n","\n","  outputs = features[\"x\"]\n","\n","  model = params['model']\n","\n","  activation = params.get(\"activation\", tf.nn.tanh)\n","\n","  print(\"Using activation - \", activation)\n","\n","  for i in range(num_hidden_layers):\n","\n","\n","      cell = model(name = 'cell'+str(i), num_units = params['hidden_layers'][i], initializer = params['initializer'])\n","      outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell, cell_bw = cell,\n","                                       inputs=outputs,\n","                                       dtype=tf.float64)\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","      outputs = outputs[1]\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","\n","      outputs = activation(outputs)\n","      outputs = tf.nn.dropout(outputs, params['dropout'])\n","\n","  #FLatten the output of LSTM layers\n","  outputs = tf.contrib.layers.flatten(outputs)\n","\n","  # FC Layer\n","  logits = tf.layers.dense(inputs=outputs, units=params['num_classes'])\n","\n","  predictions = {\n","      # Generate predictions (for PREDICT and EVAL mode)\n","      \"classes\": tf.argmax(input=logits, axis=1),\n","      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n","      # `logging_hook`.\n","      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","  }\n","\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","  # Calculate Loss (for both TRAIN and EVAL modes)\n","  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","  # Configure the Training Op (for TRAIN mode)\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","    train_op = optimizer.minimize(\n","        loss=loss,\n","        global_step=tf.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","  # Add evaluation metrics (for EVAL mode)\n","  eval_metric_ops = {\n","      \"accuracy\": tf.metrics.accuracy(\n","          labels=labels, predictions=predictions[\"classes\"])}\n","  return tf.estimator.EstimatorSpec(\n","      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDPOMaQEVgFR"},"outputs":[],"source":["def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n","\n","    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        batch_size=50,\n","        num_epochs=20,\n","        shuffle=True)\n","\n","    eeg_classifier.train(\n","        input_fn=train_input_fn,\n","        steps=100)\n","\n","    eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_test},\n","        y=y_test,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n","    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n","    print(train_results)\n","    print(test_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1688935849087,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"S78nehziViPU","outputId":"da71287a-fc0b-41d7-b6cd-7df3abecd0c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["===========================================================================\n","===========================================================================\n","Trying out initializer -  Ones\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Orthogonal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  RandomNormal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  RandomUniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  TruncatedNormal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Zeros\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Glorot Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Glorot Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  He Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  He Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Lecun Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Lecun Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n"]}],"source":["initializers = [tf.keras.initializers.Ones(), tf.keras.initializers.Orthogonal(),\n","                tf.keras.initializers.RandomNormal(), tf.keras.initializers.RandomUniform(),\n","                tf.keras.initializers.TruncatedNormal(),\n","                tf.keras.initializers.Zeros(), tf.keras.initializers.glorot_normal(), tf.keras.initializers.glorot_uniform(),\n","                tf.keras.initializers.he_normal(), tf.keras.initializers.he_uniform(), tf.keras.initializers.lecun_normal(),\n","                tf.keras.initializers.lecun_uniform()]\n","\n","names = [\"Ones\", \"Orthogonal\", \"RandomNormal\", \"RandomUniform\", \"TruncatedNormal\",\n","        \"Zeros\", \"Glorot Normal\", \"Glorot Uniform\", \"He Normal\", \"He Uniform\", \"Lecun Normal\", \"Lecun Uniform\"]\n","\n","for name, init in zip(names, initializers):\n","\n","    try:\n","\n","        print(\"=\"*75)\n","        print(\"=\"*75)\n","        print('Trying out initializer - ', str(name))\n","\n","        eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                                params = {'hidden_layers' : [64, 64], 'num_classes' : 4,\n","                                                          'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                         'dropout' : 0.5, 'activation' : tf.tanh,\n","                                                         'initializer' : init})\n","\n","\n","        calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","    except Exception as ex:\n","        print(\"*\"*75)\n","        print(\"\\nException occurred - Nan loss\")"]},{"cell_type":"markdown","metadata":{"id":"k54t9Li6byle"},"source":["###Single Script for Initializer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9666,"status":"ok","timestamp":1688941772269,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"ZwE_vK29bGTx","outputId":"be6dcff5-8d79-4245-9c25-9f4e02ecf964"},"outputs":[{"name":"stdout","output_type":"stream","text":["===========================================================================\n","===========================================================================\n","Trying out initializer -  Ones\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Orthogonal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  RandomNormal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  RandomUniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  TruncatedNormal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Zeros\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Glorot Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Glorot Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  He Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  He Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Lecun Normal\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n","===========================================================================\n","===========================================================================\n","Trying out initializer -  Lecun Uniform\n","***************************************************************************\n","\n","Exception occurred - Nan loss\n"]}],"source":["# %%\n","import tensorflow as tf\n","import numpy as np\n","import importlib\n","import matplotlib.pyplot as plt\n","import preprocessing\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# %%\n","X, y = import_data()\n","X_train,X_test,y_train,y_test = train_test_total(X, y)\n","\n","# %%\n","def rnn_model(features, labels, mode, params):\n","\n","\n","  num_hidden_layers = len(params['hidden_layers'])\n","\n","  outputs = features[\"x\"]\n","\n","  model = params['model']\n","\n","  activation = params.get(\"activation\", tf.nn.tanh)\n","\n","  print(\"Using activation - \", activation)\n","\n","  for i in range(num_hidden_layers):\n","\n","\n","      cell = model(name = 'cell'+str(i), num_units = params['hidden_layers'][i], initializer = params['initializer'])\n","      outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell, cell_bw = cell,\n","                                       inputs=outputs,\n","                                       dtype=tf.float64)\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","      outputs = outputs[1]\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","\n","      outputs = activation(outputs)\n","      outputs = tf.nn.dropout(outputs, params['dropout'])\n","\n","  #FLatten the output of LSTM layers\n","  outputs = tf.contrib.layers.flatten(outputs)\n","\n","  # FC Layer\n","  logits = tf.layers.dense(inputs=outputs, units=params['num_classes'])\n","\n","  predictions = {\n","      # Generate predictions (for PREDICT and EVAL mode)\n","      \"classes\": tf.argmax(input=logits, axis=1),\n","      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n","      # `logging_hook`.\n","      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","  }\n","\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","  # Calculate Loss (for both TRAIN and EVAL modes)\n","  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","  # Configure the Training Op (for TRAIN mode)\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","    train_op = optimizer.minimize(\n","        loss=loss,\n","        global_step=tf.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","  # Add evaluation metrics (for EVAL mode)\n","  eval_metric_ops = {\n","      \"accuracy\": tf.metrics.accuracy(\n","          labels=labels, predictions=predictions[\"classes\"])}\n","  return tf.estimator.EstimatorSpec(\n","      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n","\n","\n","# %%\n","def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n","\n","    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        batch_size=50,\n","        num_epochs=20,\n","        shuffle=True)\n","\n","    eeg_classifier.train(\n","        input_fn=train_input_fn,\n","        steps=100)\n","\n","    eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_test},\n","        y=y_test,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n","    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n","    print(train_results)\n","    print(test_results)\n","\n","# %%\n","initializers = [tf.keras.initializers.Ones(), tf.keras.initializers.Orthogonal(),\n","                tf.keras.initializers.RandomNormal(), tf.keras.initializers.RandomUniform(),\n","                tf.keras.initializers.TruncatedNormal(),\n","                tf.keras.initializers.Zeros(), tf.keras.initializers.glorot_normal(), tf.keras.initializers.glorot_uniform(),\n","                tf.keras.initializers.he_normal(), tf.keras.initializers.he_uniform(), tf.keras.initializers.lecun_normal(),\n","                tf.keras.initializers.lecun_uniform()]\n","\n","names = [\"Ones\", \"Orthogonal\", \"RandomNormal\", \"RandomUniform\", \"TruncatedNormal\",\n","        \"Zeros\", \"Glorot Normal\", \"Glorot Uniform\", \"He Normal\", \"He Uniform\", \"Lecun Normal\", \"Lecun Uniform\"]\n","\n","for name, init in zip(names, initializers):\n","\n","    try:\n","\n","        print(\"=\"*75)\n","        print(\"=\"*75)\n","        print('Trying out initializer - ', str(name))\n","\n","        eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                                params = {'hidden_layers' : [64, 64], 'num_classes' : 4,\n","                                                          'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                         'dropout' : 0.5, 'activation' : tf.tanh,\n","                                                         'initializer' : init})\n","\n","\n","        calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","    except Exception as ex:\n","        print(\"*\"*75)\n","        print(\"\\nException occurred - Nan loss\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H4yRn-LUWgrV"},"source":["\n","#Activation_Testing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaU3qzrIVlDm"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import importlib\n","import preprocessing\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpGxTGE7U_AU"},"outputs":[],"source":["X, y = import_data()\n","X_train,X_test,y_train,y_test = train_test_total(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVL7SLxwU_Cu"},"outputs":[],"source":["def rnn_model(features, labels, mode, params):\n","\n","\n","  num_hidden_layers = len(params['hidden_layers'])\n","\n","  outputs = features[\"x\"]\n","\n","  model = params['model']\n","\n","  activation = params.get(\"activation\", tf.nn.tanh)\n","\n","  print(\"Using activation - \", activation)\n","\n","  for i in range(num_hidden_layers):\n","\n","\n","      cell = model(name = 'cell'+str(i), num_units = params['hidden_layers'][i])\n","      outputs, state = tf.nn.dynamic_rnn(cell=cell,\n","                                       inputs=outputs,\n","                                       dtype=tf.float64)\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","\n","      outputs = activation(outputs)\n","      outputs = tf.nn.dropout(outputs, params['dropout'])\n","\n","  #FLatten the output of LSTM layers\n","  outputs = tf.contrib.layers.flatten(outputs)\n","\n","  # FC Layer\n","  logits = tf.layers.dense(inputs=outputs, units=params['num_classes'])\n","\n","  predictions = {\n","      # Generate predictions (for PREDICT and EVAL mode)\n","      \"classes\": tf.argmax(input=logits, axis=1),\n","      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n","      # `logging_hook`.\n","      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","  }\n","\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","  # Calculate Loss (for both TRAIN and EVAL modes)\n","  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","  # Configure the Training Op (for TRAIN mode)\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","    train_op = optimizer.minimize(\n","        loss=loss,\n","        global_step=tf.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","  # Add evaluation metrics (for EVAL mode)\n","  eval_metric_ops = {\n","      \"accuracy\": tf.metrics.accuracy(\n","          labels=labels, predictions=predictions[\"classes\"])}\n","  return tf.estimator.EstimatorSpec(\n","      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfRAlCZRU_Gg"},"outputs":[],"source":["def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n","\n","    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        batch_size=50,\n","        num_epochs=20,\n","        shuffle=True)\n","\n","    eeg_classifier.train(\n","        input_fn=train_input_fn,\n","        steps=100)\n","\n","    eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_test},\n","        y=y_test,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n","    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n","    print(train_results)\n","    print(test_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":28,"status":"error","timestamp":1688934807336,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"BUD9ws_DU_H9","outputId":"6c7f203c-d766-40a6-b1e4-3ea3dc66860a"},"outputs":[{"name":"stdout","output_type":"stream","text":["===========================================================================\n","Trying out activation -  <function relu at 0x7f74c43ed480>\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-76a40e8aa41d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n\u001b[1;32m     10\u001b[0m                                             params = {'hidden_layers' : [64, 64], 'num_classes' : 4, \n\u001b[0;32m---> 11\u001b[0;31m                                                       \u001b[0;34m'learning_rate'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                                                      'dropout' : 0.5, 'activation' : act})\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.nn' has no attribute 'rnn_cell'"]}],"source":["activations = [tf.nn.relu, tf.nn.crelu, tf.nn.elu, tf.nn.selu, tf.nn.softplus, tf.nn.softsign,\n","               tf.sigmoid, tf.tanh]\n","\n","for act in activations:\n","\n","    print(\"=\"*75)\n","    print('Trying out activation - ', str(act))\n","\n","    eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                            params = {'hidden_layers' : [64, 64], 'num_classes' : 4,\n","                                                      'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                     'dropout' : 0.5, 'activation' : act})\n","\n","\n","    calc(eeg_classifier, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"SMR_xwc6dbma"},"source":["##Activation updated from chatgpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYSWCP-SYGoF","outputId":"3becc2c9-6421-4d35-ad75-30b07992d5e2"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp05p63jj2\n"]},{"name":"stdout","output_type":"stream","text":["Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n"]},{"name":"stderr","output_type":"stream","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff995bf3ac0>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 4969, in <genexpr>\n","    input_ta = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs),\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff995c69420>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 4969, in <genexpr>\n","    input_ta = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs),\n","==================================\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7ff995cd6a40>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 4969, in <genexpr>\n","    input_ta = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 243, in wrapped\n","    return _add_should_use_warning(fn(*args, **kwargs),\n","==================================\n"]},{"name":"stdout","output_type":"stream","text":["Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n","Using activation -  <function tanh at 0x7ff9dffcf2e0>\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import importlib\n","import preprocessing\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","X, y = import_data()\n","X_train, X_test, y_train, y_test = train_test_total(X, y)\n","\n","# Convert the data to float32\n","X_train = X_train.astype(np.float32)\n","X_test = X_test.astype(np.float32)\n","\n","def rnn_model(features, labels, mode, params):\n","    num_hidden_layers = len(params['hidden_layers'])\n","    outputs = features['x']\n","    model = params['model']\n","    activation = params.get('activation', tf.nn.tanh)\n","\n","    print(\"Using activation - \", activation)\n","\n","    cells = [model(units=params['hidden_layers'][i]) for i in range(num_hidden_layers)]\n","    rnn_layer = tf.keras.layers.RNN(cells, return_sequences=True)(outputs)\n","    outputs = activation(rnn_layer)\n","    outputs = tf.keras.layers.Dropout(rate=params['dropout'])(outputs)\n","\n","    outputs = tf.keras.layers.Flatten()(outputs)\n","    logits = tf.keras.layers.Dense(units=params['num_classes'])(outputs)\n","\n","    predictions = {\n","        \"classes\": tf.argmax(logits, axis=1),\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, logits)\n","\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n","        with tf.GradientTape() as tape:\n","            logits = rnn_model(features, labels, mode, params)['logits']\n","            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, logits)\n","        trainable_vars = rnn_model.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        train_op = optimizer.minimize(loss)\n","        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n","    }\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n","def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n","    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        batch_size=50,\n","        num_epochs=20,\n","        shuffle=True)\n","    eeg_classifier.train(input_fn=train_input_fn, steps=100)\n","\n","    eval_test_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_test},\n","        y=y_test,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    eval_train_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n","    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n","    print(train_results)\n","    print(test_results)\n","\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params={'hidden_layers': [64, 64], 'num_classes': 4,\n","                                                'learning_rate': 0.001, 'model': tf.keras.layers.LSTMCell,\n","                                                'dropout': 0.5, 'activation': tf.tanh})\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n"]},{"cell_type":"markdown","metadata":{"id":"y4v2RnC6hPhw"},"source":["#RNN_LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqq7K6CPhQ9n","outputId":"3c5624e0-9c4c-408c-8f09-dbc53c667896"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-1dd12c7eaef5>:61: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1842: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From <ipython-input-4-1dd12c7eaef5>:72: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:232: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:385: StopAtStepHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From <ipython-input-4-1dd12c7eaef5>:54: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1414: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1454: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:579: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1176: get_checkpoint_mtimes (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import importlib\n","import preprocessing\n","\n","# Reload the preprocessing module if needed\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","X, y = import_data()\n","X_train, X_test, y_train, y_test = train_test_total(X, y)\n","\n","def rnn_model(features, labels, mode, params):\n","    num_hidden_layers = len(params['hidden_layers'])\n","    input_layer = features[\"x\"]\n","\n","    cell1 = tf.keras.layers.LSTM(params['hidden_layers'][0], return_sequences=True, name='cell1')\n","    outputs = cell1(input_layer)\n","    outputs = tf.nn.relu(outputs)\n","    outputs = tf.keras.layers.Dropout(rate=0.5)(outputs)\n","\n","    for i in range(num_hidden_layers - 1):\n","        return_sequences = i < num_hidden_layers - 2  # Set return_sequences=True for all layers except the last\n","        cell = tf.keras.layers.LSTM(params['hidden_layers'][i + 1], return_sequences=return_sequences, name='cell' + str(i + 2))\n","        outputs = cell(outputs)\n","        outputs = tf.nn.relu(outputs)\n","\n","    # Flatten the output of LSTM layers\n","    outputs = tf.keras.layers.Flatten()(outputs)\n","\n","    logits = tf.keras.layers.Dense(units=params['num_classes'])(outputs)\n","\n","    predictions = {\n","        \"classes\": tf.argmax(input=logits, axis=1),\n","        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","    }\n","\n","    if mode == tf.estimator.ModeKeys.PREDICT:\n","        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","    labels = tf.reshape(labels, [-1])  # Reshape labels to a 1D tensor\n","    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits))\n","    tf.summary.scalar('loss', loss)\n","\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n","        train_vars = tf.compat.v1.trainable_variables()\n","        grads = tf.gradients(loss, train_vars)\n","        grads_and_vars = zip(grads, train_vars)\n","        train_op = optimizer.apply_gradients(grads_and_vars)\n","        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","    eval_metric_ops = {\n","        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n","    }\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n","eeg_classifier = tf.estimator.Estimator(\n","    model_fn=rnn_model,\n","    model_dir=\"/model/\",\n","    params={'hidden_layers': [32, 32], 'num_classes': 4, 'learning_rate': 0.001}\n",")\n","\n","def train_input_fn():\n","    dataset = tf.data.Dataset.from_tensor_slices(({\"x\": X_train}, y_train))\n","    dataset = dataset.batch(64).repeat(20).shuffle(buffer_size=1000)\n","    return dataset\n","\n","logging_hook = tf.estimator.LoggingTensorHook(\n","    tensors={\"probabilities\": \"softmax_tensor\"},\n","    every_n_iter=50\n",")\n","\n","eeg_classifier.train(\n","    input_fn=train_input_fn,\n","    steps=100,\n","    hooks=[logging_hook]\n",")\n","\n","def eval_input_fn():\n","    dataset = tf.data.Dataset.from_tensor_slices(({\"x\": X_test}, y_test))\n","    dataset = dataset.batch(64)\n","    return dataset\n","\n","eval_results = eeg_classifier.evaluate(input_fn=eval_input_fn)\n","print(eval_results)\n"]},{"cell_type":"markdown","metadata":{"id":"u9FiWXmkndI4"},"source":["#Bidirectional"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3726928,"status":"ok","timestamp":1688947129703,"user":{"displayName":"Tamodip Das","userId":"05123256012655975106"},"user_tz":-330},"id":"_NSHgBAnndcL","outputId":"63a30bc6-1cc1-4c84-b9ea-02df45c6fbed"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-5-ba70b5d279d3>:117: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1842: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp9cmiz3kb\n","WARNING:tensorflow:From <ipython-input-5-ba70b5d279d3>:88: numpy_input_fn (from tensorflow_estimator.python.estimator.inputs.numpy_io) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:385: StopAtStepHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From <ipython-input-5-ba70b5d279d3>:34: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/rnn/legacy_cells.py:1042: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"]},{"name":"stdout","output_type":"stream","text":["Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-5-ba70b5d279d3>:73: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1414: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1417: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:232: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1454: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:579: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:910: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n"]},{"name":"stdout","output_type":"stream","text":["Using activation -  <function tanh at 0x7f44c9f75990>\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/evaluation.py:260: FinalOpsHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.keras instead.\n"]},{"name":"stdout","output_type":"stream","text":["<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpar4eskfy\n"]},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.6985646, 'loss': 0.7832359, 'global_step': 100}\n","{'accuracy': 0.64, 'loss': 0.8167752, 'global_step': 100}\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 64)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 64000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpzawpvd2s\n"]},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.7535885, 'loss': 0.6606881, 'global_step': 100}\n","{'accuracy': 0.54, 'loss': 0.9481491, 'global_step': 100}\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpaiopmjtm\n"]},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.75239235, 'loss': 0.6225511, 'global_step': 100}\n","{'accuracy': 0.66, 'loss': 0.812376, 'global_step': 100}\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 256)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 256000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 256)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 256000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 256)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 256000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpgg4fxrjl\n"]},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.761563, 'loss': 0.6178331, 'global_step': 100}\n","{'accuracy': 0.6, 'loss': 0.9036742, 'global_step': 100}\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpii_61ymc\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/rnn/legacy_cells.py:584: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"]},{"name":"stdout","output_type":"stream","text":["{'accuracy': 0.66586924, 'loss': 1.1482065, 'global_step': 100}\n","{'accuracy': 0.38, 'loss': 2.2814727, 'global_step': 100}\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","Using activation -  <function tanh at 0x7f44c9f75990>\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 1000, 128)\n","<class 'tensorflow.python.framework.ops.Tensor'>\n","(?, 128000)\n","{'accuracy': 0.75, 'loss': 0.6398369, 'global_step': 100}\n","{'accuracy': 0.56, 'loss': 1.0955027, 'global_step': 100}\n"]}],"source":["# %%\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import numpy as np\n","import importlib\n","import preprocessing\n","importlib.reload(preprocessing)\n","from preprocessing import *\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# %%\n","X, y = import_data()\n","X_train,X_test,y_train,y_test = train_test_total(X, y)\n","\n","# %%\n","def rnn_model(features, labels, mode, params):\n","\n","\n","  num_hidden_layers = len(params['hidden_layers'])\n","\n","  outputs = features[\"x\"]\n","\n","  model = params['model']\n","\n","  activation = params.get(\"activation\", tf.nn.tanh)\n","\n","  print(\"Using activation - \", activation)\n","\n","  for i in range(num_hidden_layers):\n","\n","\n","      cell = model(name = 'cell'+str(i), num_units = params['hidden_layers'][i])\n","      outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell, cell_bw = cell,\n","                                       inputs=outputs,\n","                                       dtype=tf.float64)\n","\n","      #outputs = tf.layers.batch_normalization(outputs)\n","      outputs = outputs[1]\n","      outputs = activation(outputs)\n","      outputs = tf.nn.dropout(outputs, params['dropout'])\n","\n","  print(type(outputs))\n","  print(outputs.shape)\n","  #FLatten the output of LSTM layers\n","  outputs = tf.keras.layers.Flatten()(outputs)\n","\n","  print(type(outputs))\n","  print(outputs.shape)\n","  # FC Layer\n","  logits = tf.layers.dense(inputs=outputs, units=params['num_classes'])\n","\n","  predictions = {\n","      # Generate predictions (for PREDICT and EVAL mode)\n","      \"classes\": tf.argmax(input=logits, axis=1),\n","      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n","      # `logging_hook`.\n","      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n","  }\n","\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n","\n","  # Calculate Loss (for both TRAIN and EVAL modes)\n","  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","\n","  # Configure the Training Op (for TRAIN mode)\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n","    train_op = optimizer.minimize(\n","        loss=loss,\n","        global_step=tf.train.get_global_step())\n","    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","  # Add evaluation metrics (for EVAL mode)\n","  eval_metric_ops = {\n","      \"accuracy\": tf.metrics.accuracy(\n","          labels=labels, predictions=predictions[\"classes\"])}\n","  return tf.estimator.EstimatorSpec(\n","      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","\n","\n","\n","\n","# %%\n","def calc(eeg_classifier, X_train, y_train, X_test, y_test):\n","\n","    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        batch_size=50,\n","        num_epochs=20,\n","        shuffle=True)\n","\n","    eeg_classifier.train(\n","        input_fn=train_input_fn,\n","        steps=100)\n","\n","    eval_test_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_test},\n","        y=y_test,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n","        x={\"x\": X_train},\n","        y=y_train,\n","        num_epochs=1,\n","        shuffle=False)\n","\n","    train_results = eeg_classifier.evaluate(input_fn=eval_train_fn)\n","    test_results = eeg_classifier.evaluate(input_fn=eval_test_fn)\n","    print(train_results)\n","    print(test_results)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [64, 64, 64], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [64, 64], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [128, 128], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [256, 256], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.LSTMCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [128, 128], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.BasicRNNCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","eeg_classifier = tf.estimator.Estimator(model_fn=rnn_model,\n","                                        params = {'hidden_layers' : [128, 128], 'num_classes' : 4,\n","                                                  'learning_rate' : 0.001, 'model' : tf.nn.rnn_cell.GRUCell,\n","                                                 'dropout' : 0.5, 'activation' : tf.tanh})\n","\n","\n","calc(eeg_classifier, X_train, y_train, X_test, y_test)\n","\n","# %%\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHnE1x5WvkshOhvTOmZAVf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}